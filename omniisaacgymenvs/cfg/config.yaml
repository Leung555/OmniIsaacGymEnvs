# Task name - used to pick the class to load
# task_name: Cartpole # Ant, dbAlpha, Cartpole
task_name: ${task.name}
# experiment name. defaults to name of training config
# experiment: ''
# experiment: 'dbAlpha_newGaitRew_Jvel30'
# experiment: '6legs_RBF_simMod'
# experiment: '6legs_Hebb_Wmaxnormalize'
# experiment: '6legs_walk'
# experiment: '4leg_setup'
# experiment: '2leg_setup'
# experiment: '_gaitrew'
# experiment: '_lxor'
# rewards_type: ''
# rewards_type: 'vxuy_initnoise.01'
rewards_type: '-vxuy_initnoise.01'
# rewards_type: '-vx'
# rewards_type: '-vx_randJ'
# rewards_type: 'newGaitRew'
# rewards_type: '_lxorheya_gaitrew'
# experiment: 'PPO_gaitreward'
# experiment: 'dbAlpha_PPO_tanh_sm'
# experiment: 'dbAlpha_t_PPO_tanh'
# test_env: 'normal_walk'
usd_name: ''

# Object transportation
# experiment: 'normalbox_trans'
# experiment: 'smallbox_trans'
# experiment: 'smallbox_transDR_testlrd995' # smallbox_trans(DR)
experiment: 'smallballRD_trans' # smallbox_trans(DR)
# experiment: 'box_trans_tiltL'
# experiment: 'box_trans_tiltR'
# experiment: 'nobox'
# experiment: 'box_trans_forward'
# experiment: 'ball_rolling'
# lighter object
# experiment: 'normalbox_trans.1'
# experiment: 'smallbox_trans.1'
# experiment: 'box_trans_tiltL.1'
# experiment: 'box_trans_tiltR.1'

test_env: ''
# test_env: 'smallballRD_trans'
# test_env: 'bigballRD_trans'
# test_env: 'normalbox_trans'
# test_env: 'smallboxDR_trans'
# test_env: 'tiltLbox_trans'
# test_env: 'tiltRbox_trans'
# test_env: 'nobox_trans'
# test_env: 'normalbox01_trans'
# test_env: 'smallbox01_trans'

# if set to positive integer, overrides the default number of environments
num_envs: '' # for default setup
# num_envs: 1024 # <---------------

# disables rendering
headless: True # <---------------
# test - if set, run policy in inference mode (requires setting checkpoint to load)
test: False # <---------------

wandb_activate: False # <---------------

# Model for learning, 'Feedforward' 'Hebb'
# model: 'Feedforward'  # <---------------
model: 'Hebb'
# model: 'lstm'
# model: 'seqlstm'
# model: 'rbf'
# model: 'Hebb_rbf'
# 'FF_hyper_network', 'simple_RBF_hebb', 'hyper_RBFHebb', 
# 'hyper_RBFFF', 'parallel_FF', 'parallel_Hebb'
RBFHebb_model_type: 'parallel_Hebb'

EPOCHS: 500 # <---------------
EPISODE_LENGTH: 1000 # <--------------- default: 300
SAVE_EVERY: 500 # --166, 250
USE_TRAIN_PARAMS: False # <---------------

# ARCHITECTURE = configs['Model']['HEBB']['ARCHITECTURE']['size']
# ARCHITECTURE: [4, 16, 8, 1] # for cartpole Env Test
# ARCHITECTURE: [60, 128, 64, 8] # for Ant Env Test
# ARCHITECTURE: [40, 128, 64, 8] # for Ant Env Test leg contact replace
# ARCHITECTURE: [34, 128, 64, 8] # for Ant Env Test
# ARCHITECTURE: [27, 128, 64, 8] # for Ant Env Test
# ARCHITECTURE: [102, 128, 64, 18] # for dbAlpha Env Test # old Architecture and Rewards based on  the originall PPO
# ARCHITECTURE: [72, 128, 64, 18] # for dbAlpha Env Test # old Architecture and Rewards based on  the originall PPO, leg contact replace
# ARCHITECTURE: [39, 128, 64, 18] # for dbAlpha Env Test
# ARCHITECTURE: [27, 128, 64, 18] # for dbAlpha Env Test
# ARCHITECTURE: [27, 64, 32, 18] # for dbAlpha Env Test
# ARCHITECTURE: [27, 64, 180] # for dbAlpha Env Test
# ARCHITECTURE: [19, 32, 16, 12] # for dbAlpha Env Test 4 legs
# ARCHITECTURE: [11, 16, 8, 6] # for dbAlpha Env Test 2 legs

# Locomotion
# ARCHITECTURE: [27, 64, 32, 18] # for dbAlpha Env Test
# ARCHITECTURE: [47, 64, 32, 18] # rbf hebbian parallel

# Object transport 4 legs fix hind legs
ARCHITECTURE: [19, 64, 32, 12] # 4 legs
# ARCHITECTURE: [39, 64, 32, 12] # 4 legs ebb_rbf

# Test RBF network
# RBF_ARCHITECTURE: [20, 1] # [num_kernel, num_output] cartpole 1 output
# RBF_ARCHITECTURE: [10, 1] # [num_kernel, num_output] Ant 1 output
RBF_ARCHITECTURE: [20, 18] # [num_kernel, num_output] dbAlpha 1 output
# RBF_ARCHITECTURE: [20, 12] # 4 legs object transport

# LSTM
# ARCHITECTURE: [27, 27, 18] # 6 legs walkinf
# ARCHITECTURE: [19, 19, 12] # 4 legs object trans

# seed - set to -1 to choose random seed
seed: -1
# set to True for deterministic performance
torch_deterministic: False

# set the maximum number of learning iterations to train for. overrides default per-environment setting
max_iterations: ''

## Device config
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# whether to use cpu or gpu physx
sim_device: 'gpu'
# used for gpu simulation only - device id for running sim and task if pipeline=gpu
device_id: 0
# device to run RL
rl_device: 'cuda:0'
# multi-GPU training
multi_gpu: False

## PhysX arguments
num_threads: 4 # Number of worker threads per scene used by PhysX - for CPU PhysX only.
solver_type: 1 # 0: pgs, 1: tgs

# RLGames Argumentsconfi
# used to set checkpoint path
checkpoint: ''
# checkpoint: 'runs/dbAlpha/nn/dbAlpha.pth'

# enables native livestream
enable_livestream: False
# timeout for MT script
mt_timeout: 30


wandb_group: ''
wandb_name: '' # ${train.params.config.name}
wandb_entity: ''
wandb_project: 'dbAlpha_ES_New_log' # 'omniisaacgymenvs'

# set default task and default training config based on task
defaults:
  - task: dbAlpha # Cartpole
  - train: ${task}PPO
  - hydra/job_logging: disabled

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .

# ES algorithm Standard
ES_params:
  POPSIZE: 1024 # workstation (500)
  EPISODE_LENGTH: 300 # workstation (200)
  rank_fitness: True
  antithetic: True
  learning_rate: 0.1 # 0.05:RBF
  learning_rate_decay: 0.999 # 0.9999
  sigma_init: 0.1 # 0.1
  sigma_decay: 0.999
  learning_rate_limit: 0.001
  sigma_limit: 0.0001

# # ES algorithm Test Hebb
# ES_params:
#   POPSIZE: 1024 # workstation (500)
#   EPISODE_LENGTH: 400 # workstation (200)
#   rank_fitness: True
#   antithetic: True
#   learning_rate: 0.01
#   learning_rate_decay: 0.9999 # 0.9999
#   sigma_init: 0.001 # 0.1
#   sigma_decay: 0.999
#   learning_rate_limit: 0.001
#   sigma_limit: 0.0001