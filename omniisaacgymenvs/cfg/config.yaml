
# Task name - used to pick the class to load
task_name: Cartpole # Ant, dbAlpha, Cartpole
# task_name: ${task.name}
# experiment name. defaults to name of training config
experiment: ''

# if set to positive integer, overrides the default number of environments
# num_envs: '' # for default setup
num_envs: 2 # <---------------

# disables rendering
headless: True # <---------------
# test - if set, run policy in inference mode (requires setting checkpoint to load)
test: False # <---------------

wandb_activate: False # <---------------

# Model for learning, 'Feedforward' 'Hebb'
# model: 'Feedforward'  # <---------------
model: 'Hebb'

EPOCHS: 5 # <---------------
EPISODE_LENGTH: 100 # <---------------
SAVE_EVERY: 100
USE_TRAIN_PARAMS: False # <---------------

# ARCHITECTURE = configs['Model']['HEBB']['ARCHITECTURE']['size']
ARCHITECTURE: [5, 5] # for cartpole Env Test
# ARCHITECTURE: [60, 128, 64, 8] # for Ant Env Test
# ARCHITECTURE: [102, 128, 64, 18] # for dbAlpha Env Test

# seed - set to -1 to choose random seed
seed: 42
# set to True for deterministic performance
torch_deterministic: False

# set the maximum number of learning iterations to train for. overrides default per-environment setting
max_iterations: ''

## Device config
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# whether to use cpu or gpu physx
sim_device: 'gpu'
# used for gpu simulation only - device id for running sim and task if pipeline=gpu
device_id: 0
# device to run RL
rl_device: 'cuda:0'
# multi-GPU training
multi_gpu: False

## PhysX arguments
num_threads: 4 # Number of worker threads per scene used by PhysX - for CPU PhysX only.
solver_type: 1 # 0: pgs, 1: tgs

# RLGames Argumentsconfi
# used to set checkpoint path
checkpoint: ''
# checkpoint: 'runs/dbAlpha/nn/dbAlpha.pth'

# enables native livestream
enable_livestream: False
# timeout for MT script
mt_timeout: 30


wandb_group: ''
wandb_name: ${train.params.config.name}
wandb_entity: ''
wandb_project: 'omniisaacgymenvs'

# set default task and default training config based on task
defaults:
  - task: Cartpole
  - train: ${task}PPO
  - hydra/job_logging: disabled

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .

# ES algorithm
ES_params:
  POPSIZE: 1024 # workstation (500)
  EPISODE_LENGTH: 400 # workstation (200)
  REWARD_FUNC:
    NAME: 'abs_x_D_Pen_Orient' # 'abs_y_D_Pen_Orient
    ORIENTATION_WEIGHT: 1.0
    DISTANCE_WEIGHT: 2.0
  rank_fitness: True
  antithetic: True
  learning_rate: 1.0
  learning_rate_decay: 0.9999
  sigma_init: 1
  sigma_decay: 0.999
  learning_rate_limit: 0.001
  sigma_limit: 0.01