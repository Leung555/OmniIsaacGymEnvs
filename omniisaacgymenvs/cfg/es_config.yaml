
# Task name - used to pick the class to load
task_name: ${task.name}
# experiment name. defaults to name of training config
experiment: ''

# if set to positive integer, overrides the default number of environments
num_envs: ''

# seed - set to -1 to choose random seed
seed: -1
# set to True for deterministic performance
torch_deterministic: False

# set the maximum number of learning iterations to train for. overrides default per-environment setting
max_iterations: ''

## Device config
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# whether to use cpu or gpu physx
sim_device: 'gpu'
# used for gpu simulation only - device id for running sim and task if pipeline=gpu
device_id: 0
# device to run RL
rl_device: 'cuda:0'
# multi-GPU training
multi_gpu: False

## PhysX arguments
num_threads: 4 # Number of worker threads used by PhysX - for CPU PhysX only.
solver_type: 1 # 0: pgs, 1: tgs

# RLGames Arguments
# test - if set, run policy in inference mode (requires setting checkpoint to load)
test: False
# used to set checkpoint path
checkpoint: ''
# evaluate checkpoint
evaluation: False

# disables rendering
headless: True  # <----------------------------------
# enables native livestream
enable_livestream: False
# timeout for MT script
mt_timeout: 300

# enables viewport recording
enable_recording: False
# interval between video recordings (in steps)
recording_interval: 2000
# length of the recorded video (in steps)
recording_length: 100
# fps for writing recorded video
recording_fps: 30
# directory to save recordings in
recording_dir: ''

wandb_activate: False
wandb_group: 'posC_1_2slope'
wandb_name: ${train.params.config.name}
wandb_entity: ''
wandb_project: 'my-awesome-Ant_ES_RBFHebb'

# path to a kit app file
kit_app: ''

# Warp
warp: False

# set default task and default training config based on task
defaults:
  - _self_
  - task: Cartpole
  - train: ${task}PPO
  - override hydra/job_logging: disabled

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .

# Training params
EPOCHS: 300           # <----------------------------------
SAVE_EVERY: 300        # <----------------------------------
EPISODE_LENGTH: 500   # <----------------------------------
resume_train: False

custom_rewards: False

# ES algorithm Standard
ES_params:
  # POPSIZE: 1024 # workstation (500)
  rank_fitness: True
  antithetic: True
  learning_rate: 0.1 # 0.05:RBF
  learning_rate_decay: 0.995 # 0.9999
  sigma_init: 0.1 # 0.1
  sigma_decay: 0.999
  learning_rate_limit: 0.001
  sigma_limit: 0.0001

# model 
model: 'rbf_hebb' # rbf, rbf_hebb
model_type: 'parallel_Hebb'
HEBB_ARCHITECTURE: [60, 40, 8] # [num_kernel, num_output] dbAlpha 1 output
HEBB_init_wnoise: 0.01
RBF_ARCHITECTURE: [20, 8] # [num_kernel, num_output] dbAlpha 1 output
USE_TRAIN_RBF: False
USE_TRAIN_HEBB: False
USE_TRAIN_PARAM: False

# Test params rbf_hebb network
train_rbf_path: 'Ant_test_rbf_80_99_156.2435.pickle' # flat terrain
train_hebb_path: 'Ant_test_rbf_hebb_17600_299_221.5065.pickle'  # flat terrain
# train_rbf_path: 'Ant_test_rbf_80_299_117.0807.pickle' # rough terrain
# train_hebb_path: '.5065.pickle'  # flat terrain

# Custom Terrain
terrain:
  staticFriction: 1.0  # [-]
  dynamicFriction: 1.0  # [-]
  restitution: 0.        # [-]
  # rough terrain only:
  curriculum: true
  maxInitMapLevel: 0
  mapLength: 15.
  mapWidth: 15.
  numLevels: 1 # x-axis
  numTerrains: 2 # 20, y-axis
  # terrain types: [smooth slope, rough slope, stairs up, stairs down, discrete]
  terrainProportions: [0.02, 0.9, 0.02, 0.02, 0.02]
  # tri mesh only:
  slopeTreshold: 0.5
  type: 'slope' #rough, slope
  slope_angle: 0.0